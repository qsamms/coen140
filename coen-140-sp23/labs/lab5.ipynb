{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Logistic Regression Model on raw data"
      ],
      "metadata": {
        "id": "C1SR0BbY19QK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mVIfnMD2yYFN"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# synthetic classification dataset\n",
        "from sklearn.datasets import make_classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=10, random_state=7)\n",
        "\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZBoZjOE5_ZE",
        "outputId": "dd2a5d75-3287-459f-99ec-b8e35ea40bdb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 20) (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a binary classification task and we will evaluate a LogisticRegression model after each dimensionality reduction transform.\n",
        "\n",
        "The model will be evaluated using the gold standard of repeated stratified 10-fold cross-validation. The mean and standard deviation classification accuracy across all folds and repeats will be reported."
      ],
      "metadata": {
        "id": "IVu_ensS552T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "model1 = LogisticRegression()\n",
        "\n",
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyhNY_JV1Wcd",
        "outputId": "5f16b424-b0d4-4f44-ae9d-52d2962c4220"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.824 (0.034)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Evaluate PCA with logistic regression algorithm for classification\n",
        "\n",
        "We will use a Pipeline to combine the data transform and model into an atomic unit that can be evaluated using the cross-validation procedure.\n"
      ],
      "metadata": {
        "id": "pvKbsWsi5nvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "fwD18kO76wjG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the pipeline\n",
        "steps = [('pca', PCA(n_components=10)), ('m', LogisticRegression())]\n",
        "model2 = Pipeline(steps=steps)\n",
        "\n",
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMt1h3um3f_4",
        "outputId": "56baff1c-a6de-44ae-97fb-68713065994d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.824 (0.034)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate LDA with logistic regression algorithm for classification\n",
        "\n",
        "The complete example of evaluating a model with SVD dimensionality reduction is listed below."
      ],
      "metadata": {
        "id": "ldDZjS7IgOM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "metadata": {
        "id": "WHvkPxKX6soy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the pipeline\n",
        "steps = [('svd', TruncatedSVD(n_components=10)), ('m', LogisticRegression())]\n",
        "model = Pipeline(steps=steps)\n",
        "\n",
        "# evaluate model\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report performance\n",
        "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDaVWqrKgT7X",
        "outputId": "f653cb3a-2529-429f-e3e4-40f77b92dd45"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.824 (0.034)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8bX7kPuqiopM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Choose any 2 (different from the ones shown above) dimensionality reduction techniques and train the above pipeline model using those versions of the make_classification data. Create a plot using pyplot or pandas comparing the results on the original data with all 4 Dimensionality Reduction techniques."
      ],
      "metadata": {
        "id": "TuujxaSyhzLl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QctKauk7luOr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}